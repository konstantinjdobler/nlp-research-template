im environment: 
CUDA_VISIBLE_DEVICES=0 python train.py -n "envTest2" -d ./data/test/ --model roberta-base --gpus=-1 #fÃ¼hrt alles auf GPU-0 aus
python train.py -n "envTest" -d ./data/test/ --model roberta-base --device=1 --offline #nutzt nur cpu
#im Dev-Container kann man einfach genau denselben Befehl ausfÃ¼hren

docker run --rm -it --ipc=host --gpus='"device=1"' --env WANDB_API_KEY -v "$(pwd)":/workspace -w /workspace -v "/$(pwd)/data/test:/dataContainer" oliverzim/templatetest:templateTest python train.py --gpus -1 --wandb_run_name="testrun001"
docker run --rm -it --ipc=host --gpus='"device=1"' --env WANDB_API_KEY -v "$(pwd)":/workspace -w /workspace -v "/$(pwd)/data/test:/dataContainer" oliverzim/templatetest:templateTest python train.py --gpus -1 --offline --wandb_run_name="test002"

funktioniert nicht ohne wandb_run_name
WANDB_API_KEY muss erst gesetzt werden mit expose WANDB_API_KEY=...
pwd nutzen um das aktuelle Verzeichnis zu verwenden
auÃŸerdem in dlib/frameworks/wandb.py entity auf den Nutzernamen oder Gruppennamen und project auf den Projektnamen setzen
sicherstellen das daten wirklich direkt in data-dir liegen oder mit -d spezifizieren
es wird gar kein wandb-verzeichnis erstellt!!!



--> "funktionierender" command: docker run --rm -it --ipc=host --gpus='"device=0"' --env WANDB_API_KEY -v "$(pwd)":/workspace -w /workspace -v "/$(pwd)/data/test:/dataContainer" oliverzim/templatetest:templateTest python train.py --gpus -1 -n "test_short" -d ./data/test 


Warnungen und Fehler da kein Write-Access besteht!!!!!
Warnung: zugriff auf WANDB-Ordner -> stattdessen dann tmp
Fehler: speichern von checkpoints (kann dafÃ¼r keinen neuen Ordner anlegen)



ozimmermann@gpuserver2:~/template/nlp-research-template$ docker run --rm -it --ipc=host --gpus='"device=0"' --env WANDB_API_KEY -v "$(pwd)":/workspace -w /workspace -v "/$(pwd)/data/test:/dataContainer" oliverzim/templatetest:templateTest python train.py --gpus -1 -n "test_short" -d ./data/test 

/opt/conda/envs/research/lib/python3.10/site-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 550276386
  rank_zero_warn(f"No seed found, seed set to {seed}")
Global seed set to 550276386
2023-05-23 07:12:54.969 | INFO     | __main__:main:221 - TrainingArgs(model_name_or_path='roberta-base', language_modeling_strategy='mlm', resume_training=False, checkpoint_path=None, tokenizer_path=None, data_dir='./data/test', train_file='train.txt', dev_file='dev.txt', line_by_line=False, language=None, max_sequence_length=512, overwrite_data_cache=False, conserve_disk_space=False, data_preprocessing_only=False, accelerator='auto', distributed_strategy='auto', devices=-1, workers=4, preprocessing_workers=4, precision=32, compile=False, training_goal=10000, val_frequency=500, model_log_frequency=1000, val_before_training=True, batch_size_per_device=8, effective_batch_size=None, learning_rate=5e-05, lr_warmup=1000, lr_schedule='cosine', weight_decay=0.0, gradient_clipping=None, gradient_accumulation_steps=1, train_only_embeddings=False, from_scratch=False, from_scratch_embeddings=False)
2023-05-23 07:12:54.969 | INFO     | __main__:main:221 - MiscArgs(seed=550276386, force_deterministic=False, offline=False, fast_dev_run=False, wandb_run_name='test_short', wandb_tags=[], wandb_project=None, too_many_open_files_fix=False)
wandb: WARNING Path /workspace/wandb/ wasn't writable, using system temp directory.
wandb: WARNING Path /workspace/wandb/ wasn't writable, using system temp directory
wandb: WARNING Path /workspace/wandb/ wasn't writable, using system temp directory
wandb: Currently logged in as: oliver-zimmermann. Use `wandb login --relogin` to force relogin
wandb: WARNING Path ./wandb/ wasn't writable, using system temp directory.
wandb: WARNING Path ./wandb/ wasn't writable, using system temp directory
wandb: wandb version 0.15.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /tmp/wandb/run-20230523_071255-dypa1z8v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run test_short
wandb: â­ï¸ View project at https://wandb.ai/oliver-zimmermann/template
wandb: ğŸš€ View run at https://wandb.ai/oliver-zimmermann/template/runs/dypa1z8v
2023-05-23 07:12:56.498 | INFO     | __main__:main:276 - Effective batch size 8 based on specified args1 AUTOs, 8 batch size per AUTO and1 gradient accumulation steps.
Downloading (â€¦)lve/main/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481/481 [00:00<00:00, 433kB/s]
Downloading (â€¦)olve/main/vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 899k/899k [00:00<00:00, 2.18MB/s]
Downloading (â€¦)olve/main/merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00<00:00, 1.48MB/s]
Downloading (â€¦)/main/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.36M/1.36M [00:00<00:00, 2.64MB/s]
Downloading pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 501M/501M [00:06<00:00, 80.4MB/s]
2023-05-23 07:13:10.180 | DEBUG    | src.data_loading:__init__:50 - Train file path: data/test/train.txt Dev file path: data/test/dev.txt
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2023-05-23 07:13:10.226 | INFO     | __main__:main:428 - Rank 0 | Validation before training...
2023-05-23 07:13:11.998 | DEBUG    | src.data_loading:setup:62 - Loaded tokenizer: RobertaTokenizerFast(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken("<mask>", rstrip=False, lstrip=True, single_word=False, normalized=False)}, clean_up_tokenization_spaces=True)
2023-05-23 07:13:12.016 | INFO     | src.data_loading:setup:79 - Rank 0 | Cache path: data/test/tokenized/train.txt.dev.txt.seq_len_512.tokenizerroberta-base.tokenize_fn_hash_6d835a394dd58a30.arrow
2023-05-23 07:13:12.016 | SUCCESS  | src.data_loading:setup:85 - Rank 0 | Found cached processed dataset: data/test/tokenized/train.txt.dev.txt.seq_len_512.tokenizerroberta-base.tokenize_fn_hash_6d835a394dd58a30.arrow
2023-05-23 07:13:12.021 | SUCCESS  | src.data_loading:setup:87 - Rank 0 | Loaded cached processed dataset: DatasetDict({
    train: Dataset({
        features: ['input_ids', 'attention_mask', 'special_tokens_mask'],
        num_rows: 37612
    })
    dev: Dataset({
        features: ['input_ids', 'attention_mask', 'special_tokens_mask'],
        num_rows: 68
    })
})
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 13.24it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ      Validate metric      â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚     progress/ksamples     â”‚            0.0            â”‚
â”‚         val/loss          â”‚    0.12464515119791031    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
[{'val/loss': 0.12464515119791031, 'progress/ksamples': 0.0}]
2023-05-23 07:13:13.638 | INFO     | __main__:main:432 - Rank 0 | Starting training...
2023-05-23 07:13:15.522 | DEBUG    | src.data_loading:setup:62 - Loaded tokenizer: RobertaTokenizerFast(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken("<mask>", rstrip=False, lstrip=True, single_word=False, normalized=False)}, clean_up_tokenization_spaces=True)
2023-05-23 07:13:15.540 | INFO     | src.data_loading:setup:79 - Rank 0 | Cache path: data/test/tokenized/train.txt.dev.txt.seq_len_512.tokenizerroberta-base.tokenize_fn_hash_6d835a394dd58a30.arrow
2023-05-23 07:13:15.540 | SUCCESS  | src.data_loading:setup:85 - Rank 0 | Found cached processed dataset: data/test/tokenized/train.txt.dev.txt.seq_len_512.tokenizerroberta-base.tokenize_fn_hash_6d835a394dd58a30.arrow
2023-05-23 07:13:15.545 | SUCCESS  | src.data_loading:setup:87 - Rank 0 | Loaded cached processed dataset: DatasetDict({
    train: Dataset({
        features: ['input_ids', 'attention_mask', 'special_tokens_mask'],
        num_rows: 37612
    })
    dev: Dataset({
        features: ['input_ids', 'attention_mask', 'special_tokens_mask'],
        num_rows: 68
    })
})
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
2023-05-23 07:13:15.589 | INFO     | src.model:configure_optimizers:96 - Using lr: 5e-05, weight decay: 0.0 and warmup steps: 125000

  | Name  | Type               | Params
---------------------------------------------
0 | model | RobertaForMaskedLM | 124 M 
---------------------------------------------
124 M     Trainable params
0         Non-trainable params
124 M     Total params
498.790   Total estimated model params size (MB)
Sanity Checking: 0it [00:00, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.            
Training: 0it [00:00, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
2023-05-23 07:13:15.987 | INFO     | __main__:<lambda>:368 - Total training steps: 1250000 | LR warmup steps: 125000 | Validation Frequency: 62500 | Model Log Frequencey: 125000 | Effective batch size: 8
Epoch 0:   0%|                                                                                                                                                                                                         | 0/4702 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Epoch 1:   2%|â–ˆâ–ˆâ–ˆâ–Œ                                                                                                                                                                                | 94/4702 [00:17<14:11,  5.41it/s, v_num=1z8v]^C/opt/conda/envs/research/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")
2023-05-23 07:27:59.594 | SUCCESS  | __main__:main:436 - Fit complete, starting validation...
2023-05-23 07:28:01.556 | DEBUG    | src.data_loading:setup:62 - Loaded tokenizer: RobertaTokenizerFast(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken("<mask>", rstrip=False, lstrip=True, single_word=False, normalized=False)}, clean_up_tokenization_spaces=True)
2023-05-23 07:28:01.574 | INFO     | src.data_loading:setup:79 - Rank 0 | Cache path: data/test/tokenized/train.txt.dev.txt.seq_len_512.tokenizerroberta-base.tokenize_fn_hash_6d835a394dd58a30.arrow
2023-05-23 07:28:01.574 | SUCCESS  | src.data_loading:setup:85 - Rank 0 | Found cached processed dataset: data/test/tokenized/train.txt.dev.txt.seq_len_512.tokenizerroberta-base.tokenize_fn_hash_6d835a394dd58a30.arrow
2023-05-23 07:28:01.578 | SUCCESS  | src.data_loading:setup:87 - Rank 0 | Loaded cached processed dataset: DatasetDict({
    train: Dataset({
        features: ['input_ids', 'attention_mask', 'special_tokens_mask'],
        num_rows: 37612
    })
    dev: Dataset({
        features: ['input_ids', 'attention_mask', 'special_tokens_mask'],
        num_rows: 68
    })
})
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 18.58it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ      Validate metric      â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚     progress/ksamples     â”‚     38.36800003051758     â”‚
â”‚         val/loss          â”‚   7.510255272791255e-06   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
2023-05-23 07:28:02.868 | INFO     | __main__:main:443 - Trying to save checkpoint....
2023-05-23 07:28:02.871 | ERROR    | __main__:<module>:468 - An error has been caught in function '<module>', process 'MainProcess' (1), thread 'MainThread' (140051753690944):
Traceback (most recent call last):

> File "/workspace/train.py", line 468, in <module>
    main(parsed_arg_groups)
    â”‚    â”” (TrainingArgs(model_name_or_path='roberta-base', language_modeling_strategy='mlm', resume_training=False, checkpoint_path=Non...
    â”” <function main at 0x7f5f88600790>

  File "/workspace/train.py", line 446, in main
    trainer.save_checkpoint(save_path)
    â”‚       â”‚               â”” 'template/dypa1z8v/checkpoints/last_model_ckpt.ckpt'
    â”‚       â”” <function Trainer.save_checkpoint at 0x7f5f95ad9510>
    â”” <lightning.pytorch.trainer.trainer.Trainer object at 0x7f5f8bfc6470>

  File "/opt/conda/envs/research/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1262, in save_checkpoint
    self._checkpoint_connector.save_checkpoint(filepath, weights_only=weights_only, storage_options=storage_options)
    â”‚    â”‚                     â”‚               â”‚                      â”‚                             â”” None
    â”‚    â”‚                     â”‚               â”‚                      â”” False
    â”‚    â”‚                     â”‚               â”” 'template/dypa1z8v/checkpoints/last_model_ckpt.ckpt'
    â”‚    â”‚                     â”” <function _CheckpointConnector.save_checkpoint at 0x7f5f95afedd0>
    â”‚    â”” <lightning.pytorch.trainer.connectors.checkpoint_connector._CheckpointConnector object at 0x7f5f8bcd0dc0>
    â”” <lightning.pytorch.trainer.trainer.Trainer object at 0x7f5f8bfc6470>
  File "/opt/conda/envs/research/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py", line 500, in save_checkpoint
    self.trainer.strategy.save_checkpoint(_checkpoint, filepath, storage_options=storage_options)
    â”‚    â”‚       â”‚                        â”‚            â”‚                         â”” None
    â”‚    â”‚       â”‚                        â”‚            â”” 'template/dypa1z8v/checkpoints/last_model_ckpt.ckpt'
    â”‚    â”‚       â”‚                        â”” {'epoch': 1, 'global_step': 4796, 'pytorch-lightning_version': '2.0.1.post0', 'state_dict': OrderedDict([('model.roberta.embe...
    â”‚    â”‚       â”” <property object at 0x7f5f95ae66b0>
    â”‚    â”” <lightning.pytorch.trainer.trainer.Trainer object at 0x7f5f8bfc6470>
    â”” <lightning.pytorch.trainer.connectors.checkpoint_connector._CheckpointConnector object at 0x7f5f8bcd0dc0>
  File "/opt/conda/envs/research/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 445, in save_checkpoint
    self.checkpoint_io.save_checkpoint(checkpoint, filepath, storage_options=storage_options)
    â”‚    â”‚                             â”‚           â”‚                         â”” None
    â”‚    â”‚                             â”‚           â”” 'template/dypa1z8v/checkpoints/last_model_ckpt.ckpt'
    â”‚    â”‚                             â”” {'epoch': 1, 'global_step': 4796, 'pytorch-lightning_version': '2.0.1.post0', 'state_dict': OrderedDict([('model.roberta.embe...
    â”‚    â”” <property object at 0x7f5f95c0e0c0>
    â”” <lightning.pytorch.strategies.single_device.SingleDeviceStrategy object at 0x7f5f8bcd0280>
  File "/opt/conda/envs/research/lib/python3.10/site-packages/lightning/fabric/plugins/io/torch_io.py", line 54, in save_checkpoint
    fs.makedirs(os.path.dirname(path), exist_ok=True)
    â”‚  â”‚        â”‚  â”‚    â”‚       â”” 'template/dypa1z8v/checkpoints/last_model_ckpt.ckpt'
    â”‚  â”‚        â”‚  â”‚    â”” <function dirname at 0x7f6055e4ec20>
    â”‚  â”‚        â”‚  â”” <module 'posixpath' from '/opt/conda/envs/research/lib/python3.10/posixpath.py'>
    â”‚  â”‚        â”” <module 'os' from '/opt/conda/envs/research/lib/python3.10/os.py'>
    â”‚  â”” <function LocalFileSystem.makedirs at 0x7f5f97b37640>
    â”” <fsspec.implementations.local.LocalFileSystem object at 0x7f5f88064250>
  File "/opt/conda/envs/research/lib/python3.10/site-packages/fsspec/implementations/local.py", line 54, in makedirs
    os.makedirs(path, exist_ok=exist_ok)
    â”‚  â”‚        â”‚              â”” True
    â”‚  â”‚        â”” '/workspace/template/dypa1z8v/checkpoints'
    â”‚  â”” <function makedirs at 0x7f6055e0d6c0>
    â”” <module 'os' from '/opt/conda/envs/research/lib/python3.10/os.py'>
  File "/opt/conda/envs/research/lib/python3.10/os.py", line 215, in makedirs
    makedirs(head, exist_ok=exist_ok)
    â”‚        â”‚              â”” True
    â”‚        â”” '/workspace/template/dypa1z8v'
    â”” <function makedirs at 0x7f6055e0d6c0>
  File "/opt/conda/envs/research/lib/python3.10/os.py", line 215, in makedirs
    makedirs(head, exist_ok=exist_ok)
    â”‚        â”‚              â”” True
    â”‚        â”” '/workspace/template'
    â”” <function makedirs at 0x7f6055e0d6c0>
  File "/opt/conda/envs/research/lib/python3.10/os.py", line 225, in makedirs
    mkdir(name, mode)
    â”‚     â”‚     â”” 511
    â”‚     â”” '/workspace/template'
    â”” <built-in function mkdir>

PermissionError: [Errno 13] Permission denied: '/workspace/template'
Traceback (most recent call last):
  File "/workspace/train.py", line 468, in <module>
    main(parsed_arg_groups)
  File "/opt/conda/envs/research/lib/python3.10/site-packages/loguru/_logger.py", line 1251, in catch_wrapper
    return function(*args, **kwargs)
  File "/workspace/train.py", line 446, in main
    trainer.save_checkpoint(save_path)
  File "/opt/conda/envs/research/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1262, in save_checkpoint
    self._checkpoint_connector.save_checkpoint(filepath, weights_only=weights_only, storage_options=storage_options)
  File "/opt/conda/envs/research/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py", line 500, in save_checkpoint
    self.trainer.strategy.save_checkpoint(_checkpoint, filepath, storage_options=storage_options)
  File "/opt/conda/envs/research/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 445, in save_checkpoint
    self.checkpoint_io.save_checkpoint(checkpoint, filepath, storage_options=storage_options)
  File "/opt/conda/envs/research/lib/python3.10/site-packages/lightning/fabric/plugins/io/torch_io.py", line 54, in save_checkpoint
    fs.makedirs(os.path.dirname(path), exist_ok=True)
  File "/opt/conda/envs/research/lib/python3.10/site-packages/fsspec/implementations/local.py", line 54, in makedirs
    os.makedirs(path, exist_ok=exist_ok)
  File "/opt/conda/envs/research/lib/python3.10/os.py", line 215, in makedirs
    makedirs(head, exist_ok=exist_ok)
  File "/opt/conda/envs/research/lib/python3.10/os.py", line 215, in makedirs
    makedirs(head, exist_ok=exist_ok)
  File "/opt/conda/envs/research/lib/python3.10/os.py", line 225, in makedirs
    mkdir(name, mode)
PermissionError: [Errno 13] Permission denied: '/workspace/template'
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: | 0.488 MB of 0.488 MB uploaded (0.000 MB deduped)
wandb: Run history:
wandb:               epoch â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆ
wandb:        lr-AdamW/pg1 â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:        lr-AdamW/pg2 â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   progress/ksamples â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:          train/loss â–ˆâ–‡â–†â–„â–â–†â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            val/loss â–ˆâ–
wandb: 
wandb: Run summary:
wandb:               epoch 1
wandb:        lr-AdamW/pg1 0.0
wandb:        lr-AdamW/pg2 0.0
wandb:   progress/ksamples 38.368
wandb:          train/loss 1e-05
wandb: trainer/global_step 4796
wandb:            val/loss 1e-05
wandb: 
wandb: ğŸš€ View run test_short at: https://wandb.ai/oliver-zimmermann/template/runs/dypa1z8v
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20230523_071255-dypa1z8v/logs